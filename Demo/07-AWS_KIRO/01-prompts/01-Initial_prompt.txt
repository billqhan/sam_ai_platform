Please assist in Generating Python code for a data processing pipeline.  (This is adapted from my prompt for the AWS Diagram MPC Server)
I would like to do the following: 

Title: "AI Powered RFP Response Agent"

1 - Data Retrieval 
Retrieve opportunity information from SAM.gov using the SAM API using a lambda function
* External API (SAM.GOV data)  'https://api.sam.gov/prod/opportunities/v2/search'
* AWS Lambda Function ("sam-gov-daily-download")
* Cron Job calls lambda functions 1x every day
* Result: "SAM Opportunities.json" (JSON contains multiple opportunity records)

2 - Raw Data Storage
Store data "SAM Opportunities.json" (JSON contains multiple opportunity records) 
* Data is written to s3 bucket "sam-data-in" 
* Logs are written to "sam-download-files-logs"

3 - SAM Opportunity Splitter Processor
Split the "sam opportunities json file" into the individual opportunity files
* Trigger for Lambda Function "sam-json-processor" is on s3 bucket "sam-data-in".
* Function splits "SAM Opportunities.json" into multiple files, 1 file per opportunity. (the "opportunity_number prefix" is the title of the json file.)
* Resulting files go into s3 bucket "sam-extracted-json-resources" 
* In each JSON file, there is a "resouce link" list of 0 or many files. The are also downloaded, and the "opportunity_number prefix" is added to all the files. 

4 - SQS "sqs-sam-json-messages" 
As data is processed by the "SAM Opportunity Splitter Processor" , the data is put into an SQS queue
* Trigger is on the bucket "sam-extracted-json-resources".
* All new .json files are added to the SQS queue "sqs-sam-json-messages"

5- Use AWS Bedrock Knowledge Base,  using AWS S3 storage as database
* Create KB named  "Company Information KB" 
* Source files for KB  Files are placed in point to s3 bucket "sam-company-info"


6 - Parse  Opportunities Info and Generate Match Reports
* Perform  "BEDROCK AI  PROCESSING" - 
* Lambda function "sam-sqs-geneate-match-reports" read the SQS "sqs-sam-json-messages" when new message are added
* Batch size = 1, Maximum Concurrenty between values of 1 and N
* All results are writtend to S3 bucket "sam-matching-out-sqs"
* Makes calls to BOTH Bedrock LLM: "Get Opportunity Description" and "Calculate Opportunity to Company Match"


6A - Call to Bedrock LLM  - "Get Opportunity Info"
* CALLED BY SQS "sqs-sam-json-messages" 
* LLM Agent reads the Opportunity JSON and assosciated downloaded files, and extacts out "key information" from the SAM opportunity 
* Data will be passed FROM "Get Opportunity Info" TO "Calculate Company Match"

6B - Call to Bedrock LLM - "Calculate Company Match"
* CALLED BY SQS "sqs-sam-json-messages" 
* Generates "match score"
    - Uses "key information" from the SAM opportunity
    - Looks for matching data in "Company Information"
    - Uses LLM Prompt to determine match score
    - AWS Bedrock LLM will perform the match using a detailed LLM prompt "match Criteria" 

* Results are written to S3 bucket "sam-matching-out-sqs"
    - Opportunities Above match score threshold are sent to "matched" folder
    - Opportunities Below match score threshold are sent to "not_matched" folder
    - Individual Summaries are written to "runs" folder

6C - Connected to DATABASE "Company Information KB" 
* 2 WAY CONNECTION WITH Bedrock LLM  - "Calculate Company Match" (
    - Arrow from "Company Information KB" to  "Calculate Company Match"
    - Arrow from "Calculate Company Match"  to  "Company Information KB"
* This is the RAG which contains skills about the company

(
Please put step 7 in a BOX "Run Results".
This BOX is BELOW the s3 bucket  "sam-matching-out-sqs".
"sam-lambda-every-5min-summarizer" and "sam-merge-and-archive-result-logs" should be in the box
 )

7 - Aggregate Daily Run Results (FINAL STEP)
* EventBridge event "sam-lambda-every-5min-summarizer" runs every 5 minutes
* "sam-lambda-every-5min-summarizer" calls Lambda Function "sam-merge-and-archive-result-logs"

    - Lambda Function "sam-merge-and-archive-result-logs" READS and WRITES TO  S3 bucket "sam-matching-out-sqs".
    - 
    - Lambda Function "sam-merge-and-archive-result-logs" READS all of the run results in the  S3 bucket "sam-matching-out-sqs" "runs" folder (result files that were generated in the last 5 minutes)

    - Lambda Function "sam-merge-and-archive-result-logs" GENERATES Summary of the run results and WRITES in the  S3 bucket "sam-matching-out-sqs"  to a new "results" file (prefixed with the data and timestamp)

    - Individual (non summary) files are written to are sent to S3 bucket "sam-matching-out-sqs" "archive" folder 

8 - Generate Summary WebPage 
- AWS S3 website at  'sam-website'
- Write Summary_YYYYMMDD.html
- Log files are extracted from "result-logs" runs folder 
- A sample webpage from another application can be provided to summarize resuults.